
# Team Number â€“ Project Title

## Team Info
- 22471A05A0 â€” Kanumuri Narendra ( [LinkedIn](https://linkedin.com/in/xxxxxxxxxx) )
_Work Done: xxxxxxxxxx_

- 22471A05B1 â€” Nallamekala Vignesh ( [LinkedIn](https://linkedin.com/in/xxxxxxxxxx) )
_Work Done: xxxxxxxxxx_

- 22471A05B8 â€”  Peddipaka Udaykiran ( [LinkedIn](https://linkedin.com/in/xxxxxxxxxx) )
_Work Done: xxxxxxxxxx_


---

## Abstract
This paper presents a deep learning-based frame
work named Smart Apparel Narrator, designed to automatically
generate meaningful captions for fashion apparel in both images
and videos. The system integrates a ConvNeXt-Large encoder
for extracting detailed apparel features and an LSTM decoder
for coherent caption generation. For video sequences, the model
applies frame-level feature alignment to capture dynamic apparel
movements. A filtered dataset containing over 1,000 annotated
apparel images and clips across 26 fashion categories was used
for experimentation. The proposed method achieved a BLEU-1
score of 0.946, outperforming standard CNNâ€“LSTM captioning
baselines and demonstrating high descriptive accuracy. This
framework offers significant potential for automated e-commerce
tagging, assistive narration for visually impaired users, and
fashion video analysis. Future extensions include attention-based
captioning and transformer architectures for enhanced context
retention. The Smart Apparel Narrator framework closes the
loop between computer vision and fashion understanding by
allowing machines to annotate clothes with human-like accuracy.
Different from conventional captioning systems designed for
common scenes, however, this method is solely concentrating
on fashion features like texture, pattern, material, and design
properties. The performance of the model showcases its flexibility
towards various apparel types while ensuring language fluency.
Through effective feature learning and context alignment, it
can produce context-aware and descriptive captions. It can
enable personalized fashion advice, digital catalog management,
and accessibility solutions. The study shows that combining
deep vision models with sequential text generation can enable
substantial improvement in user engagement with visual retail
information.

---

## Paper Reference (Inspiration)
ðŸ‘‰ **[Paper Title Image and Video Captioning for
Apparels Using Deep Learning
  â€“ Author Names GOVIND AGARWAL,KRITIKA JINDAL,ABISHI CHOWDHURY,VISHAL K. SINGH,AMRIT PAL
 ](Paper URL here)**
Original conference/IEEE paper used as inspiration for the model.

---

## Our Improvement Over Existing Paper
xxxxxxxxxx

---

## About the Project
Give a simple explanation of:
- What your project does
- Why it is useful
- General project workflow (input â†’ processing â†’ model â†’ output)

---

## Dataset Used
ðŸ‘‰ **[Dataset Name](Dataset URL)**

**Dataset Details:**
xxxxxxxxxx

---

## Dependencies Used
xxxxxxxxxx, xxxxxxxxxx, xxxxxxxxxx ...

---

## EDA & Preprocessing
xxxxxxxxxx

---

## Model Training Info
xxxxxxxxxx

---

## Model Testing / Evaluation
xxxxxxxxxx

---

## Results
xxxxxxxxxx

---

## Limitations & Future Work
xxxxxxxxxx

---

## Deployment Info
xxxxxxxxxx

---
