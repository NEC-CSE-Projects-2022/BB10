
# Team Number â€“ Project Title

ğŸ‘¥ Team Info
- 22471A05A0 â€” Kanumuri Narendra ( [LinkedIn](https://www.linkedin.com/in/narendra-kanumuri-6b4649276/) )
Work Done: Designed the overall system architecture and implemented the ConvNeXtâ€“LSTM captioning pipeline. Developed image/video preprocessing, feature extraction, caption generation, and end-to-end model training.

- 22471A05B1 â€” Nallamekala Vignesh ( [LinkedIn](https://www.linkedin.com/in/nallamekala-vignesh-9b992a361/) )
Work Done: Performed dataset collection, cleaning, and preprocessing. Handled caption text processing, tokenization, padding, and conducted model evaluation using BLEU metrics and performance analysis.

- 22471A05B8 â€”  Peddipaka Udaykiran ( [LinkedIn](https://www.linkedin.com/in/uday-kiran-65bb88282/) )
Work Done: Implemented video frame extraction using OpenCV, integrated feature extraction and inference pipeline, optimized training workflow, and supported deployment and visualization modules.


---

## Abstract
This paper presents a deep learning-based frame
work named Smart Apparel Narrator, designed to automatically
generate meaningful captions for fashion apparel in both images
and videos. The system integrates a ConvNeXt-Large encoder
for extracting detailed apparel features and an LSTM decoder
for coherent caption generation. For video sequences, the model
applies frame-level feature alignment to capture dynamic apparel
movements. A filtered dataset containing over 1,000 annotated
apparel images and clips across 26 fashion categories was used
for experimentation. The proposed method achieved a BLEU-1
score of 0.946, outperforming standard CNNâ€“LSTM captioning
baselines and demonstrating high descriptive accuracy. This
framework offers significant potential for automated e-commerce
tagging, assistive narration for visually impaired users, and
fashion video analysis. Future extensions include attention-based
captioning and transformer architectures for enhanced context
retention. The Smart Apparel Narrator framework closes the
loop between computer vision and fashion understanding by
allowing machines to annotate clothes with human-like accuracy.
Different from conventional captioning systems designed for
common scenes, however, this method is solely concentrating
on fashion features like texture, pattern, material, and design
properties. The performance of the model showcases its flexibility
towards various apparel types while ensuring language fluency.
Through effective feature learning and context alignment, it
can produce context-aware and descriptive captions. It can
enable personalized fashion advice, digital catalog management,
and accessibility solutions. The study shows that combining
deep vision models with sequential text generation can enable
substantial improvement in user engagement with visual retail
information.

---

## Paper Reference (Inspiration)
ğŸ‘‰ **[Paper Title Image and Video Captioning for
Apparels Using Deep Learning
  â€“ Author Names GOVIND AGARWAL,KRITIKA JINDAL,ABISHI CHOWDHURY,VISHAL K. SINGH,AMRIT PAL
 ](https://ieeexplore.ieee.org/document/10636169)**



## Our Improvement Over Existing Paper
âŒ Removes frame-independent captioning

Unlike the existing Image and Video Captioning for Apparels Using Deep Learning system, which captions each detected object/frame independently after YOLO-based detection, our model avoids isolated predictions. Instead, it generates context-aware captions, improving coherence across frames and reducing inconsistent descriptions.
ğŸ“¦ Larger & cleaner dataset

The previous work used 863 base apparel images (with augmentation).
Our system uses 1000+ carefully filtered and annotated images and video clips, providing:

better diversity

reduced noise

stronger generalization

improved real-world performance
ğŸ”„ Adds frame-level feature alignment (video continuity)

Instead of treating video frames separately, we introduce frame-level feature alignment:

preserves temporal consistency

reduces caption flickering

produces smooth narration for moving apparel

This results in stable and natural video descriptions.
ğŸ§  Uses stronger visual encoder (ConvNeXt-Large specialization)

We employ ConvNeXt-Large as a dedicated encoder to capture:

texture

fabric

pattern

color

design details

This enables fine-grained fashion understanding, not just object detection.
âœï¸ Improves linguistic fluency

Enhanced caption preprocessing + tokenization + sequence learning:

cleaner vocabulary

better grammar

fewer repetitive words

longer meaningful sentences

Captions become human-like and descriptive, instead of short labels.
Balanced evaluation (not just unigram accuracy)

The existing model focuses mainly on high BLEU-1.
Our system achieves balanced BLEU-1 to BLEU-4:

0.946 / 0.932 / 0.924 / 0.917


This shows:

better multi-word consistency

stronger sentence structure

improved contextual correctness


---
##ğŸ§© About the Project

This project implements a deep learningâ€“based apparel captioning system capable of automatically generating natural language descriptions for clothing images and videos within a single framework.

Users can upload an image to get a descriptive caption or provide a video to receive continuous frame-by-frame narration.

The system is useful for applications such as e-commerce product description automation, accessibility support for visually impaired users, fashion video understanding, and smart retail analytics.
ğŸ” Workflow
Input Image / Video â†’ Preprocessing â†’ ConvNeXt (Feature Extraction) â†’ LSTM (Caption Generation) â†’ Output Caption

Input is taken either as an image or video.

Images/frames are resized, normalized, and cleaned during preprocessing.

ConvNeXt-Large extracts deep visual features from apparel.

LSTM decoder generates captions word-by-word using learned language context.

Output is returned as a descriptive text caption.


---

##ğŸ“Š Dataset Used
ğŸ‘‰ **[Dataset Name:Fashion Product Images Dataset](Dataset URL:https://www.kaggle.com/datasets/paramaggarwal/fashion-product-images-dataset)**

ğŸ—‚ Dataset Details
ğŸŸ¢ Fashion Product Images Dataset

Contains ~44,000+ real-world fashion product images along with detailed metadata.
It provides a large and balanced benchmark for training and evaluating apparel captioning, classification, and retrieval systems

##ğŸ§° Dependencies Used
ğŸ Python â€“ Core programming language used for system development and training
ğŸ‘ï¸ OpenCV â€“ Image loading, resizing, preprocessing, and video frame extraction
ğŸ”¥ TensorFlow / Keras â€“ Deep learning framework for building and training the captioning model
ğŸ§  ConvNeXt-Large â€“ Visual feature extraction from apparel images
ğŸ” LSTM â€“ Sequential caption generation from image features
ğŸ“Š NumPy â€“ Numerical computation and feature vector operations
ğŸ“ Pandas â€“ Dataset handling, CSV processing, and metadata cleaning
ğŸ“ Tokenizer â€“ Text cleaning, word indexing, and sequence padding
ğŸ“‰ NLTK (BLEU) â€“ Caption quality evaluation using BLEU-1 to BLEU-4 metrics
ğŸ¨ Matplotlib â€“ Training loss and performance visualization
ğŸ’» Google Colab / Jupyter â€“ Training and experimentation environment

##ğŸ” EDA & Preprocessing
ğŸ–¼ï¸ All images are converted to RGB format to maintain uniformity across the dataset
ğŸ“ Images are resized to 299Ã—299 / 512Ã—512 pixels to ensure compatibility with the CNN encoder
ğŸ§¹ Corrupted, missing, and duplicate image files are removed during data cleaning
ğŸ“Š Dataset distribution is analyzed to check category balance and reduce class imbalance
ğŸ·ï¸ Captions are cleaned by removing punctuation, converting to lowercase, and adding start/end tokens
ğŸ”¢ Text is tokenized and padded to convert words into numerical sequences for LSTM training
ğŸ§  ConvNeXt-Large extracts deep visual feature vectors instead of using raw images directly
ğŸ¥ Videos are split into frames and each frame is preprocessed individually for caption generation

##ğŸ§ª Model Training Info
ğŸ§  ConvNeXt-Large generates deep visual feature embeddings from apparel images for semantic understanding
ğŸ” LSTM decoder learns sequential language patterns to generate captions word-by-word
ğŸ“ Images are resized and normalized before training to ensure stable and faster convergence
ğŸ“ Tokenized captions are padded to fixed-length sequences for efficient batch training
ğŸ¯ Cross-Entropy Loss measures the difference between predicted and actual words during caption generation
âš¡ Adam optimizer updates model weights efficiently for faster and smoother learning
ğŸ”„ Teacher Forcing is used to guide the decoder with correct previous words during training for improved accuracy
ğŸ“Š BLEU-1 to BLEU-4 metrics evaluate caption quality and sentence-level coherence
ğŸ¥ Video frames are processed individually and captions are generated continuously for smooth narration

##ğŸ§¾ Model Testing / Evaluation
ğŸ“ Metrics Used:
ğŸ“Š BLEU-1 â€“ Measures single-word (unigram) caption accuracy
ğŸ“Š BLEU-2 â€“ Evaluates two-word phrase consistency
ğŸ“Š BLEU-3 â€“ Checks longer phrase coherence
ğŸ“Š BLEU-4 â€“ Measures full sentence fluency and contextual correctness
ğŸ†š Compared With:
ğŸ”¹ CNNâ€“LSTM baseline (Show & Tell model)
ğŸ”¹ Attention-based captioning models
ğŸ”¹ Transformer-based captioning approaches
ğŸ“ˆ Evaluation Process
ğŸ–¼ï¸ Tested on unseen apparel images and videos to measure generalization
ğŸ¥ Frame-by-frame evaluation ensures smooth and consistent video narration
ğŸ§  Generated captions are compared with ground-truth captions using BLEU scores
âš–ï¸ Performance demonstrates improved semantic understanding and balanced sentence-level accuracy



##ğŸ† Results
âœ… Apparel Image Captioning
ğŸ¯ BLEU-1 Score: 0.946
ğŸ¥‡ BLEU-2 Score: 0.932
ğŸ“Š BLEU-3 Score: 0.924
ğŸ… BLEU-4 Score: 0.917
âœ… Video Captioning
ğŸ¥ Smooth frame-by-frame narration with reduced caption flickering
âš¡ ~150 ms average processing time per frame for detection + caption generation
ğŸ§  Maintains temporal consistency across consecutive frames
ğŸ“ˆ Performance Highlights
âœï¸ Generates human-like, descriptive apparel captions (color, texture, pattern, design)
ğŸ“¦ Better generalization using larger and cleaner dataset
ğŸš€ Suitable for real-time e-commerce and assistive applications
ğŸ†š Outperforms traditional CNNâ€“LSTM and attention-based captioning baselines in overall caption fluency and multi-word consistency

##âš ï¸ Limitations & Future Work
ğŸ’» Requires high GPU resources for training and real-time video caption generation
ğŸ“‰ Caption quality may reduce for very complex scenes with multiple overlapping apparel items
ğŸ¥ Frame-by-frame processing can increase latency for long or high-resolution videos
ğŸ—‚ Dataset size is moderate; larger datasets could further improve generalization
ğŸŒ Future Enhancements Include:
âš¡ Real-time captioning optimization for faster inference
ğŸ“¦ Training on larger and more diverse fashion datasets
ğŸ§  Integration of attention/transformer-based captioning models for better context understanding
ğŸ–¥ Development of a web/mobile interface for user-friendly deployment
ğŸŒ Multilingual caption generation for broader accessibility
â™¿ Text-to-speech integration for visually impaired users

##ğŸŒ Deployment Info
ğŸ–¥ Implemented using a Python-based backend for model training and inference
ğŸ§  ConvNeXt-Large + LSTM models deployed for real-time apparel caption generation
âš¡ GPU acceleration (CUDA-enabled systems) used for faster feature extraction and caption prediction
ğŸ¥ OpenCV handles real-time image and video frame processing
ğŸŒ Can be deployed using Flask or FastAPI for web-based captioning services
ğŸ“¦ Supports batch image/video uploads for scalable e-commerce or retail applications
âœ¨ Project By:
ğŸ‘¨â€ğŸ’» Narendra Kanumuri
ğŸ“ Smart Apparel Narrator â€“ Deep Learning-Based Apparel Image & Video Captioning System
